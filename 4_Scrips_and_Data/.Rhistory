# Author:
# Juan Bettinelli,
# Script to be used in the Paper "Quantification of methane emissions in Hamburg using a network of FTIR spectrometers and an inverse modeling approach"
# Data from the Hamburg campaign 2021-2022.
# This Script is used for a Keeling analysed of the data collected in Hamburg Geomatikum in 2021.
# The script produces a timeline of the total Methan concentration
#Last edit: 22.05.2023
# Declare librarys used
library(ggplot2)
library(hexbin)
library(gridExtra)
library(dplyr)
library(plotly)
library(rio)
library(gridExtra)
library(grid)
library(pracma)
library(patchwork)
# library(ggplot2)
# library(hexbin)
# library(dplyr)
library(lubridate)
library(plotly)
library(rio)
library(cowplot)
#------------------------------------------------------------------
# Function to perform calculations on selected rows
calculate_and_append <- function(TotalData, start_time) {
# Select rows within the first 12 hours
selected_rows <- TotalData %>%
filter(between(DateTime, start_time, start_time + hours(12)))
# Keeling Analyse for total data of the champagne Time series
# For C13
c13C_Line <- lm(d13C.VPDB ~ c13C, TotalData )
c13C_coef <- coef(summary(c13C_Line))[, "Estimate"]
c13C_se <- coef(summary(c13C_Line))[, "Std. Error"]
# For H2
c2H_Line <- lm(d2H.VPDB ~ c2H, TotalData )
c2H_coef <- coef(summary(c2H_Line))[, "Estimate"]
c2H_se <- coef(summary(c2H_Line))[, "Std. Error"]
return(c13C_coef)
}
#------------------------------------------------------------------
# Set Working Directory
setwd("/Users/juanbettinelli/Documents/Uni/MasterThesis/4_Scrips_and_Data")
# Set Starting and Finish Time
StartTime <- as.POSIXct('2021-08-01 22:03:00',
format = "%Y-%m-%d %H:%M:%S",
tz ="utc")
# Start Time: 2021-08-01 22:03:00
FinishTime <- as.POSIXct('2022-03-29 00:00:00',
format = "%Y-%m-%d %H:%M:%S",
tz ="utc")
# Total Timeseries: 2022-03-29 00:00:00
# Hamburg Campagne Timeseries: 2021-09-06 00:00:00
# Hamburg Campaine #2: 2021-09-17 10:21:00
########### Read the CSV File #############
# Read the CSV File
TotalData <- import("4_Data/OutputData/CombineMeteorologicalData.csv")
# format the Date 'UTC'
TotalData$UTC <- as.POSIXct(as.character(TotalData$UTC),
format = "%Y-%m-%d %H:%M:%S",
tz = "UTC")
# Convert the format of 'X.CH4.' to numeric
TotalData$X.CH4. <- as.numeric(TotalData$X.CH4.)
# Filter out all the dated that are outside the selected Strating and Finish time of the campaign
TotalData <- filter(TotalData, TotalData$UTC > StartTime & TotalData$UTC < FinishTime, .preserve = FALSE)
# Remove Empty Cells n data frame
TotalData <- TotalData[!is.na(TotalData$UTC),]
# Calculate 1/Mole Fraction for C13 & H2
TotalData$c13C <- 1/TotalData$X.CH4..13C
TotalData$c2H <- 1/TotalData$X.CH4..2H
EMPA_csv <- import("4_Data/9_EMPA/ts_sim_concDelta_HH_GEO_IFS_FP-C1.csv")
# format the Date 'UTC'
EMPA_csv$dtm.end <- as.POSIXct(as.character(EMPA_csv$dtm.end),
format = "%Y-%m-%d %H:%M:%S",
tz = "UTC")
# Convert the format of 'X.CH4.' to numeric
EMPA_csv$CH4 <- as.numeric(EMPA_csv$CH4)
# Filter out all the dated that are outside the selected Strating and Finish time of the campaign
EMPA_csv <- filter(EMPA_csv, EMPA_csv$dtm.end > StartTime & EMPA_csv$dtm.end < FinishTime, .preserve = FALSE)
# Remove Empty Cells n data frame
EMPA_csv <- EMPA_csv[!is.na(EMPA_csv$dtm.end),]
# Calculate 1/Mole Fraction for C13 & H2
EMPA_csv$c13C <- 1/EMPA_csv$CH4
EMPA_csv$c2H <- 1/EMPA_csv$CH4
############# Find the Peaks and the  Create a dataframe with only the Peaks ###########
# #Select the Data from Data frame with CH4 Concentration
# CH4Data <- TotalData[complete.cases(TotalData[ , "X.CH4."]),c("UTC", "X.CH4.")]
##### Find Loweres 15%
#Select the Data from Dataframe with CH4 Concentration
CH4Data <- TotalData[complete.cases(TotalData[ , "X.CH4."]),c("UTC", "X.CH4.")]
# Sort the dataset in ascending order
sorted_data <- sort(CH4Data$X.CH4.)
# Determine the number of observations corresponding to the lowest 15% of the dataset
n_lowest <- round(length(sorted_data) * 0.15)
# Use the head() function to extract the lowest 15% of the dataset
lowest_15_percent <- max(head(sorted_data, n_lowest))
######
# Find the Peaks in the timeline
# The Peaks criteria can be selected hire, The comments give some usefull ones
CH4_Peaks <- as.data.frame(findpeaks(CH4Data$X.CH4., minpeakheight = 2250, minpeakdistance = 30, threshold = 5, sortstr=TRUE)) # Strict peaks: CH4Data$X.CH4.,minpeakheight = 2400, minpeakdistance = 15, threshold = 5, sortstr=TRUE) ,medium peaks: CH4Data$X.CH4.,minpeakheight = 2100, minpeakdistance = 25, threshold = 5, sortstr=TRUE , Peak like in the paper: (CH4Data$X.CH4.,minpeakheight = lowest_15_percent, minpeakdistance = 5, threshold = 5, sortstr=TRUE)
# Format the Peak Dataframe
names(CH4_Peaks) <- c("X.CH4.", "UTC", "UTC_Beginning", "UTC_Ending")
CH4_Peaks$UTC_Beginning <- CH4Data[CH4_Peaks$UTC_Beginning,"UTC"]
CH4_Peaks$UTC_Ending <- CH4Data[CH4_Peaks$UTC_Ending,"UTC"]
CH4_Peaks$UTC <- CH4Data[CH4_Peaks$UTC,"UTC"]
# Find all the values in the TotalData Dataframe 12h before and 12 after the peak. The Time can be changed hire as needed
# than it findes the lowest value (troth) in that timeline
for (k in 1:nrow(CH4_Peaks)){
testDFUp <- filter(TotalData, TotalData$UTC > (CH4_Peaks[k,2]) & TotalData$UTC < (CH4_Peaks[k,2]+12*60*60), .preserve = FALSE)
testDFUp <- testDFUp[complete.cases(testDFUp[ , "X.CH4."]),]
CH4_Up <- as.data.frame(findpeaks(-testDFUp$X.CH4., npeaks = 1, sortstr=TRUE))
if (nrow(CH4_Up) == 0){
CH4_Up <- data.frame(NA, tail(testDFUp$UTC, n = 1), NA, NA )
names(CH4_Up) <- c("X.CH4.", "UTC", "UTC_Beginning", "UTC_Ending")
}
else{
names(CH4_Up) <- c("X.CH4.", "UTC", "UTC_Beginning", "UTC_Ending")
CH4_Up$UTC <- testDFUp[CH4_Up$UTC,"UTC"]
}
CH4_Peaks[k,"UTC_Ending"] <- CH4_Up[1, "UTC"]
testDFDown <- filter(TotalData, TotalData$UTC > (CH4_Peaks[k,2]-12*60*60) & TotalData$UTC < (CH4_Peaks[k,2]), .preserve = FALSE)
testDFDown <- testDFDown[complete.cases(testDFDown[ , "X.CH4."]),]
CH4_Down <- as.data.frame(findpeaks(-testDFDown$X.CH4., npeaks = 1, sortstr=TRUE))
if (nrow(CH4_Down) == 0){
CH4_Down <- data.frame(NA, tail(testDFDown$UTC, n = 1), NA,NA )
names(CH4_Down) <- c("X.CH4.", "UTC", "UTC_Beginning", "UTC_Ending")
}
else {
names(CH4_Down) <- c("X.CH4.", "UTC", "UTC_Beginning", "UTC_Ending")
CH4_Down$UTC <- testDFDown[CH4_Down$UTC,"UTC"]
}
CH4_Peaks[k,"UTC_Beginning"] <- CH4_Down[1, "UTC"]
}
# ##### Find Loweres 15%
# #Select the Data from Dataframe with CH4 Concentration
# CH4Data <- TotalData[complete.cases(TotalData[ , "X.CH4."]),c("UTC", "X.CH4.")]
#
# # Sort the dataset in ascending order
# sorted_data <- sort(CH4Data$X.CH4.)
#
# # Determine the number of observations corresponding to the lowest 15% of the dataset
# n_lowest <- round(length(sorted_data) * 0.15)
#
# # Use the head() function to extract the lowest 15% of the dataset
# lowest_15_percent <- max(head(sorted_data, n_lowest))
# ######
#
# # Find the Peaks in the Remaining timeline
# CH4_Peaks <- as.data.frame(findpeaks(CH4Data$X.CH4.,minpeakheight = lowest_15_percent, minpeakdistance = 5, threshold = 5, sortstr=TRUE)) # Strict peaks: CH4Data$X.CH4.,minpeakheight = 2400, minpeakdistance = 15, threshold = 5, sortstr=TRUE) ,medium peaks: CH4Data$X.CH4.,minpeakheight = 2100, minpeakdistance = 25, threshold = 5, sortstr=TRUE , Peak like in the paper: (CH4Data$X.CH4.,minpeakheight = lowest_15_percent, minpeakdistance = 5, threshold = 5, sortstr=TRUE)
#
# # Format the Peak Data frame 'CH4_Peaks'
# # Rename the Columns
# names(CH4_Peaks) <- c("X.CH4.", "UTC", "UTC_Beginning", "UTC_Ending")
# # Replace the Index with Timestemps 'UTC'
# CH4_Peaks$UTC_Beginning <- CH4Data[CH4_Peaks$UTC_Beginning,"UTC"]
# CH4_Peaks$UTC_Ending <- CH4Data[CH4_Peaks$UTC_Ending,"UTC"]
# CH4_Peaks$UTC <- CH4Data[CH4_Peaks$UTC,"UTC"]
# # Find the average during the Peak, (Average all values that lay between the Peak beginning and Peak End)
# # Get all Columns Names from 'TotalData
# Heads <- colnames(TotalData)
# # Remove empty Columns
# Heads <- Heads[-1]
# Heads <- Heads[-16]
# Create Data frame with only peaks
Total_Peaks <- data.frame()
# for-loop over rows
for(i in 1:nrow(CH4_Peaks)) {
Single_Peak <- TotalData[TotalData$UTC >= CH4_Peaks[i,"UTC_Beginning"] & TotalData$UTC <= CH4_Peaks[i,"UTC_Ending"], ]
Total_Peaks <- rbind(Total_Peaks,Single_Peak)
}
Total_Peaks <- Total_Peaks[complete.cases(Total_Peaks[ , "X.CH4."]),]
No_Peaks <- subset(TotalData, UTC = Total_Peaks$UTC) ###### check if it works!!!!
No_Peaks <- No_Peaks[complete.cases(No_Peaks[ , "X.CH4."]),]
################ Keeling analyse ##############
result_df <- data.frame(DateTime = as.POSIXct(character()), mean_value = numeric())
while (StartTime < FinishTime) {
# Call the function to perform calculations and append to the result dataframe
result <- calculate_and_append(TotalData, StartTime)
result_df <- bind_rows(result_df, result)
# Move 1 hour forward
StartTime <- StartTime + hours(1)
}
# Author:
# Juan Bettinelli,
# Script to be used in the Paper "Quantification of methane emissions in Hamburg using a network of FTIR spectrometers and an inverse modeling approach"
# Data from the Hamburg campaign 2021-2022.
# This Script is used for a Keeling analysed of the data collected in Hamburg Geomatikum in 2021.
# The script produces a timeline of the total Methan concentration
#Last edit: 22.05.2023
# Declare librarys used
library(ggplot2)
library(hexbin)
library(gridExtra)
library(dplyr)
library(plotly)
library(rio)
library(gridExtra)
library(grid)
library(pracma)
library(patchwork)
# library(ggplot2)
# library(hexbin)
# library(dplyr)
library(lubridate)
library(plotly)
library(rio)
library(cowplot)
#------------------------------------------------------------------
# Function to perform calculations on selected rows
calculate_and_append <- function(TotalData, start_time) {
# Select rows within the first 12 hours
# selected_rows <- TotalData %>%
#   filter(between(DateTime, start_time, start_time + hours(12)))
TotalData <- filter(TotalData, TotalData$UTC > start_time & TotalData$UTC < start_time + hours(12), .preserve = FALSE)
# Keeling Analyse for total data of the champagne Time series
# For C13
c13C_Line <- lm(d13C.VPDB ~ c13C, TotalData )
c13C_coef <- coef(summary(c13C_Line))[, "Estimate"]
c13C_se <- coef(summary(c13C_Line))[, "Std. Error"]
# For H2
c2H_Line <- lm(d2H.VPDB ~ c2H, TotalData )
c2H_coef <- coef(summary(c2H_Line))[, "Estimate"]
c2H_se <- coef(summary(c2H_Line))[, "Std. Error"]
return(c13C_coef)
}
#------------------------------------------------------------------
# Set Working Directory
setwd("/Users/juanbettinelli/Documents/Uni/MasterThesis/4_Scrips_and_Data")
# Set Starting and Finish Time
StartTime <- as.POSIXct('2021-08-01 22:03:00',
format = "%Y-%m-%d %H:%M:%S",
tz ="utc")
# Start Time: 2021-08-01 22:03:00
FinishTime <- as.POSIXct('2022-03-29 00:00:00',
format = "%Y-%m-%d %H:%M:%S",
tz ="utc")
# Total Timeseries: 2022-03-29 00:00:00
# Hamburg Campagne Timeseries: 2021-09-06 00:00:00
# Hamburg Campaine #2: 2021-09-17 10:21:00
########### Read the CSV File #############
# Read the CSV File
TotalData <- import("4_Data/OutputData/CombineMeteorologicalData.csv")
# format the Date 'UTC'
TotalData$UTC <- as.POSIXct(as.character(TotalData$UTC),
format = "%Y-%m-%d %H:%M:%S",
tz = "UTC")
# Convert the format of 'X.CH4.' to numeric
TotalData$X.CH4. <- as.numeric(TotalData$X.CH4.)
# Filter out all the dated that are outside the selected Strating and Finish time of the campaign
TotalData <- filter(TotalData, TotalData$UTC > StartTime & TotalData$UTC < FinishTime, .preserve = FALSE)
# Remove Empty Cells n data frame
TotalData <- TotalData[!is.na(TotalData$UTC),]
# Calculate 1/Mole Fraction for C13 & H2
TotalData$c13C <- 1/TotalData$X.CH4..13C
TotalData$c2H <- 1/TotalData$X.CH4..2H
EMPA_csv <- import("4_Data/9_EMPA/ts_sim_concDelta_HH_GEO_IFS_FP-C1.csv")
# format the Date 'UTC'
EMPA_csv$dtm.end <- as.POSIXct(as.character(EMPA_csv$dtm.end),
format = "%Y-%m-%d %H:%M:%S",
tz = "UTC")
# Convert the format of 'X.CH4.' to numeric
EMPA_csv$CH4 <- as.numeric(EMPA_csv$CH4)
# Filter out all the dated that are outside the selected Strating and Finish time of the campaign
EMPA_csv <- filter(EMPA_csv, EMPA_csv$dtm.end > StartTime & EMPA_csv$dtm.end < FinishTime, .preserve = FALSE)
# Remove Empty Cells n data frame
EMPA_csv <- EMPA_csv[!is.na(EMPA_csv$dtm.end),]
# Calculate 1/Mole Fraction for C13 & H2
EMPA_csv$c13C <- 1/EMPA_csv$CH4
EMPA_csv$c2H <- 1/EMPA_csv$CH4
############# Find the Peaks and the  Create a dataframe with only the Peaks ###########
# #Select the Data from Data frame with CH4 Concentration
# CH4Data <- TotalData[complete.cases(TotalData[ , "X.CH4."]),c("UTC", "X.CH4.")]
##### Find Loweres 15%
#Select the Data from Dataframe with CH4 Concentration
CH4Data <- TotalData[complete.cases(TotalData[ , "X.CH4."]),c("UTC", "X.CH4.")]
# Sort the dataset in ascending order
sorted_data <- sort(CH4Data$X.CH4.)
# Determine the number of observations corresponding to the lowest 15% of the dataset
n_lowest <- round(length(sorted_data) * 0.15)
# Use the head() function to extract the lowest 15% of the dataset
lowest_15_percent <- max(head(sorted_data, n_lowest))
######
# Find the Peaks in the timeline
# The Peaks criteria can be selected hire, The comments give some usefull ones
CH4_Peaks <- as.data.frame(findpeaks(CH4Data$X.CH4., minpeakheight = 2250, minpeakdistance = 30, threshold = 5, sortstr=TRUE)) # Strict peaks: CH4Data$X.CH4.,minpeakheight = 2400, minpeakdistance = 15, threshold = 5, sortstr=TRUE) ,medium peaks: CH4Data$X.CH4.,minpeakheight = 2100, minpeakdistance = 25, threshold = 5, sortstr=TRUE , Peak like in the paper: (CH4Data$X.CH4.,minpeakheight = lowest_15_percent, minpeakdistance = 5, threshold = 5, sortstr=TRUE)
# Format the Peak Dataframe
names(CH4_Peaks) <- c("X.CH4.", "UTC", "UTC_Beginning", "UTC_Ending")
CH4_Peaks$UTC_Beginning <- CH4Data[CH4_Peaks$UTC_Beginning,"UTC"]
CH4_Peaks$UTC_Ending <- CH4Data[CH4_Peaks$UTC_Ending,"UTC"]
CH4_Peaks$UTC <- CH4Data[CH4_Peaks$UTC,"UTC"]
# Find all the values in the TotalData Dataframe 12h before and 12 after the peak. The Time can be changed hire as needed
# than it findes the lowest value (troth) in that timeline
for (k in 1:nrow(CH4_Peaks)){
testDFUp <- filter(TotalData, TotalData$UTC > (CH4_Peaks[k,2]) & TotalData$UTC < (CH4_Peaks[k,2]+12*60*60), .preserve = FALSE)
testDFUp <- testDFUp[complete.cases(testDFUp[ , "X.CH4."]),]
CH4_Up <- as.data.frame(findpeaks(-testDFUp$X.CH4., npeaks = 1, sortstr=TRUE))
if (nrow(CH4_Up) == 0){
CH4_Up <- data.frame(NA, tail(testDFUp$UTC, n = 1), NA, NA )
names(CH4_Up) <- c("X.CH4.", "UTC", "UTC_Beginning", "UTC_Ending")
}
else{
names(CH4_Up) <- c("X.CH4.", "UTC", "UTC_Beginning", "UTC_Ending")
CH4_Up$UTC <- testDFUp[CH4_Up$UTC,"UTC"]
}
CH4_Peaks[k,"UTC_Ending"] <- CH4_Up[1, "UTC"]
testDFDown <- filter(TotalData, TotalData$UTC > (CH4_Peaks[k,2]-12*60*60) & TotalData$UTC < (CH4_Peaks[k,2]), .preserve = FALSE)
testDFDown <- testDFDown[complete.cases(testDFDown[ , "X.CH4."]),]
CH4_Down <- as.data.frame(findpeaks(-testDFDown$X.CH4., npeaks = 1, sortstr=TRUE))
if (nrow(CH4_Down) == 0){
CH4_Down <- data.frame(NA, tail(testDFDown$UTC, n = 1), NA,NA )
names(CH4_Down) <- c("X.CH4.", "UTC", "UTC_Beginning", "UTC_Ending")
}
else {
names(CH4_Down) <- c("X.CH4.", "UTC", "UTC_Beginning", "UTC_Ending")
CH4_Down$UTC <- testDFDown[CH4_Down$UTC,"UTC"]
}
CH4_Peaks[k,"UTC_Beginning"] <- CH4_Down[1, "UTC"]
}
# ##### Find Loweres 15%
# #Select the Data from Dataframe with CH4 Concentration
# CH4Data <- TotalData[complete.cases(TotalData[ , "X.CH4."]),c("UTC", "X.CH4.")]
#
# # Sort the dataset in ascending order
# sorted_data <- sort(CH4Data$X.CH4.)
#
# # Determine the number of observations corresponding to the lowest 15% of the dataset
# n_lowest <- round(length(sorted_data) * 0.15)
#
# # Use the head() function to extract the lowest 15% of the dataset
# lowest_15_percent <- max(head(sorted_data, n_lowest))
# ######
#
# # Find the Peaks in the Remaining timeline
# CH4_Peaks <- as.data.frame(findpeaks(CH4Data$X.CH4.,minpeakheight = lowest_15_percent, minpeakdistance = 5, threshold = 5, sortstr=TRUE)) # Strict peaks: CH4Data$X.CH4.,minpeakheight = 2400, minpeakdistance = 15, threshold = 5, sortstr=TRUE) ,medium peaks: CH4Data$X.CH4.,minpeakheight = 2100, minpeakdistance = 25, threshold = 5, sortstr=TRUE , Peak like in the paper: (CH4Data$X.CH4.,minpeakheight = lowest_15_percent, minpeakdistance = 5, threshold = 5, sortstr=TRUE)
#
# # Format the Peak Data frame 'CH4_Peaks'
# # Rename the Columns
# names(CH4_Peaks) <- c("X.CH4.", "UTC", "UTC_Beginning", "UTC_Ending")
# # Replace the Index with Timestemps 'UTC'
# CH4_Peaks$UTC_Beginning <- CH4Data[CH4_Peaks$UTC_Beginning,"UTC"]
# CH4_Peaks$UTC_Ending <- CH4Data[CH4_Peaks$UTC_Ending,"UTC"]
# CH4_Peaks$UTC <- CH4Data[CH4_Peaks$UTC,"UTC"]
# # Find the average during the Peak, (Average all values that lay between the Peak beginning and Peak End)
# # Get all Columns Names from 'TotalData
# Heads <- colnames(TotalData)
# # Remove empty Columns
# Heads <- Heads[-1]
# Heads <- Heads[-16]
# Create Data frame with only peaks
Total_Peaks <- data.frame()
# for-loop over rows
for(i in 1:nrow(CH4_Peaks)) {
Single_Peak <- TotalData[TotalData$UTC >= CH4_Peaks[i,"UTC_Beginning"] & TotalData$UTC <= CH4_Peaks[i,"UTC_Ending"], ]
Total_Peaks <- rbind(Total_Peaks,Single_Peak)
}
Total_Peaks <- Total_Peaks[complete.cases(Total_Peaks[ , "X.CH4."]),]
No_Peaks <- subset(TotalData, UTC = Total_Peaks$UTC) ###### check if it works!!!!
No_Peaks <- No_Peaks[complete.cases(No_Peaks[ , "X.CH4."]),]
################ Keeling analyse ##############
result_df <- data.frame(DateTime = as.POSIXct(character()), mean_value = numeric())
while (StartTime < FinishTime) {
# Call the function to perform calculations and append to the result dataframe
result <- calculate_and_append(TotalData, StartTime)
result_df <- bind_rows(result_df, result)
# Move 1 hour forward
StartTime <- StartTime + hours(1)
}
View(result_df)
result_df <- data.frame()
while (StartTime < FinishTime) {
# Call the function to perform calculations and append to the result dataframe
result <- calculate_and_append(TotalData, StartTime)
result_df <- bind_rows(result_df, result)
# Move 1 hour forward
StartTime <- StartTime + hours(1)
}
View(result_df)
#------------------------------------------------------------------
# Function to perform calculations on selected rows
calculate_and_append <- function(TotalData, start_time) {
# Select rows within the first 12 hours
# selected_rows <- TotalData %>%
#   filter(between(DateTime, start_time, start_time + hours(12)))
TotalData <- filter(TotalData, TotalData$UTC > start_time & TotalData$UTC < start_time + hours(12), .preserve = FALSE)
# Keeling Analyse for total data of the champagne Time series
# For C13
c13C_Line <- lm(d13C.VPDB ~ c13C, TotalData )
c13C_coef <- coef(summary(c13C_Line))[, "Estimate"]
c13C_se <- coef(summary(c13C_Line))[, "Std. Error"]
# For H2
c2H_Line <- lm(d2H.VPDB ~ c2H, TotalData )
c2H_coef <- coef(summary(c2H_Line))[, "Estimate"]
c2H_se <- coef(summary(c2H_Line))[, "Std. Error"]
return(c13C_coef[[1]])
}
#----------------
result_df <- data.frame(UTC = as.POSIXct(character()), c13C_coef = numeric())
while (StartTime < FinishTime) {
# Call the function to perform calculations and append to the result dataframe
result <- calculate_and_append(TotalData, StartTime)
result_df <- bind_rows(result_df, result)
# Move 1 hour forward
StartTime <- StartTime + hours(1)
}
View(result_df)
# Call the function to perform calculations and append to the result dataframe
result <- calculate_and_append(TotalData, StartTime)
# Function to perform calculations on selected rows
calculate_and_append <- function(TotalData, start_time) {
# Select rows within the first 12 hours
# selected_rows <- TotalData %>%
#   filter(between(DateTime, start_time, start_time + hours(12)))
TotalData <- filter(TotalData, TotalData$UTC > start_time & TotalData$UTC < start_time + hours(12), .preserve = FALSE)
# Keeling Analyse for total data of the champagne Time series
# For C13
c13C_Line <- lm(d13C.VPDB ~ c13C, TotalData )
c13C_coef <- coef(summary(c13C_Line))[, "Estimate"]
c13C_se <- coef(summary(c13C_Line))[, "Std. Error"]
# For H2
c2H_Line <- lm(d2H.VPDB ~ c2H, TotalData )
c2H_coef <- coef(summary(c2H_Line))[, "Estimate"]
c2H_se <- coef(summary(c2H_Line))[, "Std. Error"]
result <- data.frame(UTC = start_time, c13C_coef = c13C_coef[[1]])
return(result)
}
result_df <- data.frame(UTC = as.POSIXct(character()), c13C_coef = numeric())
while (StartTime < FinishTime) {
# Call the function to perform calculations and append to the result dataframe
result <- calculate_and_append(TotalData, StartTime)
result_df <- bind_rows(result_df, result)
# Move 1 hour forward
StartTime <- StartTime + hours(1)
}
# Call the function to perform calculations and append to the result dataframe
result <- calculate_and_append(TotalData, StartTime)
# Function to perform calculations on selected rows
calculate_and_append <- function(TotalData, start_time) {
# Select rows within the first 12 hours
# selected_rows <- TotalData %>%
#   filter(between(DateTime, start_time, start_time + hours(12)))
TotalData <- filter(TotalData, TotalData$UTC > start_time & TotalData$UTC < start_time + hours(12), .preserve = FALSE)
# Keeling Analyse for total data of the champagne Time series
# For C13
c13C_Line <- lm(d13C.VPDB ~ c13C, TotalData )
c13C_coef <- coef(summary(c13C_Line))[, "Estimate"]
c13C_se <- coef(summary(c13C_Line))[, "Std. Error"]
# For H2
c2H_Line <- lm(d2H.VPDB ~ c2H, TotalData )
c2H_coef <- coef(summary(c2H_Line))[, "Estimate"]
c2H_se <- coef(summary(c2H_Line))[, "Std. Error"]
result <- data.frame(start_time, c13C_coef[[1]])
return(result)
}
# Function to perform calculations on selected rows
calculate_and_append <- function(TotalData, start_time) {
# Select rows within the first 12 hours
# selected_rows <- TotalData %>%
#   filter(between(DateTime, start_time, start_time + hours(12)))
TotalData <- filter(TotalData, TotalData$UTC > start_time & TotalData$UTC < start_time + hours(12), .preserve = FALSE)
# Keeling Analyse for total data of the champagne Time series
# For C13
c13C_Line <- lm(d13C.VPDB ~ c13C, TotalData )
c13C_coef <- coef(summary(c13C_Line))[, "Estimate"]
c13C_se <- coef(summary(c13C_Line))[, "Std. Error"]
# For H2
c2H_Line <- lm(d2H.VPDB ~ c2H, TotalData )
c2H_coef <- coef(summary(c2H_Line))[, "Estimate"]
c2H_se <- coef(summary(c2H_Line))[, "Std. Error"]
result <- c(start_time, c13C_coef[[1]])
return(result)
#------------------------------------------------------------------
# Function to perform calculations on selected rows
calculate_and_append <- function(TotalData, start_time) {
# Select rows within the first 12 hours
# selected_rows <- TotalData %>%
#   filter(between(DateTime, start_time, start_time + hours(12)))
TotalData <- filter(TotalData, TotalData$UTC > start_time & TotalData$UTC < start_time + hours(12), .preserve = FALSE)
# Keeling Analyse for total data of the champagne Time series
# For C13
c13C_Line <- lm(d13C.VPDB ~ c13C, TotalData )
c13C_coef <- coef(summary(c13C_Line))[, "Estimate"]
c13C_se <- coef(summary(c13C_Line))[, "Std. Error"]
# For H2
c2H_Line <- lm(d2H.VPDB ~ c2H, TotalData )
c2H_coef <- coef(summary(c2H_Line))[, "Estimate"]
c2H_se <- coef(summary(c2H_Line))[, "Std. Error"]
result <- c(start_time, c13C_coef[[1]])
return(result)
}
calculate_and_append <- function(TotalData, start_time) {
# Select rows within the first 12 hours
# selected_rows <- TotalData %>%
#   filter(between(DateTime, start_time, start_time + hours(12)))
TotalData <- filter(TotalData, TotalData$UTC > start_time & TotalData$UTC < start_time + hours(12), .preserve = FALSE)
# Keeling Analyse for total data of the champagne Time series
# For C13
c13C_Line <- lm(d13C.VPDB ~ c13C, TotalData )
c13C_coef <- coef(summary(c13C_Line))[, "Estimate"]
c13C_se <- coef(summary(c13C_Line))[, "Std. Error"]
# For H2
c2H_Line <- lm(d2H.VPDB ~ c2H, TotalData )
c2H_coef <- coef(summary(c2H_Line))[, "Estimate"]
c2H_se <- coef(summary(c2H_Line))[, "Std. Error"]
result <- c(start_time, c13C_coef[[1]])
return(result)
}
